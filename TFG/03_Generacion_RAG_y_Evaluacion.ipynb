{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b80317",
   "metadata": {},
   "source": [
    "# Fase 3: GeneraciÃ³n Aumentada (RAG) y EvaluaciÃ³n de Modelos\n",
    "\n",
    "## 1. IntroducciÃ³n\n",
    "Tras haber completado la **IngenierÃ­a de Datos** (Fase 1: Ingesta y Limpieza) y validado el **Motor de RecuperaciÃ³n** (Fase 2: IndexaciÃ³n en Elasticsearch), entramos en la etapa final y mÃ¡s crÃ­tica del sistema: la **GeneraciÃ³n**.\n",
    "\n",
    "En este notebook, desplegaremos la inteligencia del sistema utilizando la GPU disponible (RTX 4090). El objetivo es conectar el \"cerebro\" (Modelos de Lenguaje) con la \"memoria\" (Nuestra base de datos de noticias) para sintetizar respuestas veraces y actualizadas.\n",
    "\n",
    "## 2. Objetivos del Notebook\n",
    "Siguiendo las directrices del TFG, este entorno estÃ¡ diseÃ±ado para ser modular y permitir la comparaciÃ³n experimental entre diferentes arquitecturas. Los pasos tÃ©cnicos serÃ¡n:\n",
    "\n",
    "1.  **ConfiguraciÃ³n del Entorno de Inferencia:** InicializaciÃ³n de librerÃ­as de Deep Learning (`transformers`, `accelerate`) y verificaciÃ³n de acceso a la GPU.\n",
    "2.  **Despliegue de Modelos (SLM vs LLM):**\n",
    "    * Carga de un **Small Language Model (SLM)**: Empezaremos con *Phi-3* (3.8B parÃ¡metros) para validar la hipÃ³tesis de que un modelo pequeÃ±o con buen contexto puede rendir bien.\n",
    "    * PreparaciÃ³n para **Large Language Models (LLM)**: El cÃ³digo serÃ¡ reutilizable para cargar modelos mayores (como *Llama-3* o *Mistral*) posteriormente.\n",
    "3.  **FusiÃ³n RAG (Retrieval-Augmented Generation):** ImplementaciÃ³n de la funciÃ³n `ask_rag(question)` que orquesta el flujo completo:\n",
    "    * *Input Usuario* $\\rightarrow$ *BÃºsqueda en Elasticsearch* $\\rightarrow$ *ConstrucciÃ³n del Prompt* $\\rightarrow$ *GeneraciÃ³n*.\n",
    "4.  **IngenierÃ­a de Prompts:** DiseÃ±o de un \"System Prompt\" estricto que obligue al modelo a basarse exclusivamente en las noticias recuperadas para mitigar alucinaciones.\n",
    "\n",
    "## 3. Arquitectura del Flujo\n",
    "El sistema no permite que el LLM improvise. El flujo de datos es unidireccional y forzado:\n",
    "> **Pregunta** $\\xrightarrow{BM25}$ **Noticias (Contexto)** $\\rightarrow$ **[INSTRUCCIÃ“N + CONTEXTO + PREGUNTA]** $\\rightarrow$ **LLM** $\\rightarrow$ **Respuesta Verificada**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2b6a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javierruiz/miniconda3/envs/environment/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware detectado: NVIDIA GeForce RTX 4090\n",
      "Cargando modelo: microsoft/Phi-3-mini-4k-instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195/195 [00:01<00:00, 125.96it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Â¡Modelo SLM cargado y listo para inferencia!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# 1. ConfiguraciÃ³n de Hardware\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Hardware detectado: {torch.cuda.get_device_name(0) if device == 'cuda' else 'CPU'}\")\n",
    "\n",
    "# 2. SelecciÃ³n de Modelo (Empezamos con un SLM para la prueba rÃ¡pida)\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "print(f\"Cargando modelo: {MODEL_ID}...\")\n",
    "\n",
    "# 3. Carga del Tokenizer y el Modelo \n",
    "# Importante: Ponemos trust_remote_code=False para usar la versiÃ³n estable instalada\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",             # Distribuye automÃ¡ticamente en la GPU\n",
    "    torch_dtype=torch.float16    # Usamos precisiÃ³n media para ahorrar VRAM y ganar velocidad\n",
    ")\n",
    "\n",
    "# 4. Crear Pipeline de GeneraciÃ³n\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\nðŸš€ Â¡Modelo SLM cargado y listo para inferencia!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73c2c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'do_sample', 'temperature', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=150) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generando respuesta...\n",
      "\n",
      "==================================================\n",
      " La inflaciÃ³n econÃ³mica es un fenÃ³meno que se refiere a la tasa a la cual el nivel general de precios de bienes y servicios estÃ¡ aumentando, y, por consiguiente, el poder adquisitivo estÃ¡ disminuyendo. En otras palabras, con la inflaciÃ³n, cada unidad de moneda compra menos bienes y servicios. Esto puede deberse a diversos factores, como un aumento en la oferta de dinero, un aumento en los costos de producciÃ³n, o una disminuciÃ³n en la demanda de bienes y servicios. La inflaciÃ³n es un indicador importante de la salud econÃ³mica de\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. Prueba de Inferencia (Sin RAG todavÃ­a)\n",
    "# Vamos a ver si el \"cerebro\" funciona por sÃ­ solo.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Â¿PodrÃ­as explicarme brevemente quÃ© es la inflaciÃ³n econÃ³mica?\"},\n",
    "]\n",
    "\n",
    "# ConfiguraciÃ³n de generaciÃ³n\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 150,     # Que no escriba una novela, solo un pÃ¡rrafo\n",
    "    \"return_full_text\": False, # Que no repita la pregunta\n",
    "    \"temperature\": 0.1,        # Creatividad baja (queremos datos, no poesÃ­a)\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "print(\" Generando respuesta...\")\n",
    "output = pipe(messages, **generation_args)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(output[0]['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b788314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PING EXITOSO: Versiones sincronizadas (8.12.0).\n",
      "   Conectado a: valencia\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Ahora que las versiones coinciden, la configuraciÃ³n es estÃ¡ndar y sencilla\n",
    "es = Elasticsearch(\"http://127.0.0.1:9250\")\n",
    "\n",
    "try:\n",
    "    if es.ping():\n",
    "        print(\"âœ… PING EXITOSO: Versiones sincronizadas (8.12.0).\")\n",
    "        print(f\"   Conectado a: {es.info()['name']}\")\n",
    "    else:\n",
    "        print(\"âŒ El servidor no responde. Â¿EstÃ¡ el tÃºnel abierto?\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f944f6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sistema RAG (k=1) configurado para match perfecto.\n"
     ]
    }
   ],
   "source": [
    "def ask_rag(query, top_k=1): # <--- Forzamos k=1 como pide el profesor\n",
    "    \"\"\"\n",
    "    FunciÃ³n RAG para TFG: Recupera 1 noticia y genera respuesta basada estrictamente en ella.\n",
    "    \"\"\"\n",
    "    # --- A. BÃšSQUEDA (Retrieval de 1 sola noticia) ---\n",
    "    search_payload = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"title^5\", \"body\"], # Mucho peso al tÃ­tulo para el 'match perfecto'\n",
    "                \"fuzziness\": 0 # 0 = Match perfecto (sin errores tipogrÃ¡ficos permitidos)\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"title\", \"body\", \"date\", \"source\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = es.search(index=\"noticias_tfg\", body=search_payload)\n",
    "        hits = response['hits']['hits']\n",
    "    except Exception as e:\n",
    "        return f\"Error en Elasticsearch: {e}\"\n",
    "    \n",
    "    if not hits:\n",
    "        return \"âš ï¸ NO ENCONTRADO: No hay ninguna noticia que coincida exactamente con la bÃºsqueda.\"\n",
    "\n",
    "    # --- B. EXTRACCIÃ“N DE LA NOTICIA ÃšNICA ---\n",
    "    doc = hits[0]['_source']\n",
    "    contexto_unico = f\"\"\"\n",
    "    TITULO: {doc.get('title')}\n",
    "    FECHA: {doc.get('date', 'Desconocida')}\n",
    "    FUENTE: {doc.get('source', 'Desconocida')}\n",
    "    CONTENIDO: {doc.get('body')}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- C. PROMPT PARA MITIGAR ALUCINACIONES ---\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Eres un sistema de verificaciÃ³n de datos (Fact-Checking). \n",
    "        Tu objetivo es responder a la pregunta usando ÃšNICAMENTE el texto que te proporciono abajo.\n",
    "\n",
    "        REGLAS CRÃTICAS:\n",
    "        1. Si la respuesta no aparece en el texto, responde exactamente: \"No tengo informaciÃ³n suficiente en mis archivos\".\n",
    "        2. No utilices conocimiento externo.\n",
    "        3. No menciones otras noticias que no sean la proporcionada.\n",
    "\n",
    "        ### TEXTO DE REFERENCIA:\n",
    "        {contexto_unico}\n",
    "        \n",
    "        ### PREGUNTA:\n",
    "        {query}\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # --- D. GENERACIÃ“N ---\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False, # Ponemos False para que sea determinista (siempre responda igual)\n",
    "        temperature=0.0, # Temperatura 0 para evitar inventos\n",
    "    )\n",
    "    \n",
    "    return outputs[0]['generated_text']\n",
    "\n",
    "print(\" Sistema RAG (k=1) configurado para match perfecto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db88476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=150) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RESPUESTA SLM (SIN RAG) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Â¿QuÃ© aranceles puso Trump a China?'}, {'role': 'assistant', 'content': ' El presidente Donald Trump impuso aranceles a China como parte de la Ley de ProtecciÃ³n de la Industria Automotriz (Auto Industry Trade Policy Act), conocida simplemente como \"Aranceles a la China\". Estos aranceles se enmarcaron en el contexto de una larga disputa comercial entre Estados Unidos y China que buscaba proteger la industria automotriz frente a la competencia china. La implementaciÃ³n de estos aranceles, junto con otros sanciones comerciales, fue un punto de inflexiÃ³n en las relaciones econÃ³micas entre los dos paÃ­ses.'}]\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "--- RESPUESTA SISTEMA RAG (k=1) ---\n",
      "[{'role': 'user', 'content': '\\n        Eres un sistema de verificaciÃ³n de datos (Fact-Checking). \\n        Tu objetivo es responder a la pregunta usando ÃšNICAMENTE el texto que te proporciono abajo.\\n\\n        REGLAS CRÃTICAS:\\n        1. Si la respuesta no aparece en el texto, responde exactamente: \"No tengo informaciÃ³n suficiente en mis archivos\".\\n        2. No utilices conocimiento externo.\\n        3. No menciones otras noticias que no sean la proporcionada.\\n\\n        ### TEXTO DE REFERENCIA:\\n        \\n    TITULO: Aranceles de Trump a China alcanzan el 145%\\n    FECHA: 2025-04-10 10:48:17.000000\\n    FUENTE: ExcÃ©lsior\\n    CONTENIDO: La Casa Blanca precisÃ³ que el aumento del 125% de aranceles a China se suma al 20% vigente desde principios de marzo.\\n    \\n\\n        ### PREGUNTA:\\n        Â¿QuÃ© aranceles puso Trump a China?\\n        '}, {'role': 'assistant', 'content': ' El presidente Trump puso aranceles a China que suman un 125% al 20% vigente desde principios de marzo.'}]\n"
     ]
    }
   ],
   "source": [
    "pregunta_tfg = \"Â¿QuÃ© aranceles puso Trump a China?\"\n",
    "\n",
    "# 1. Prueba SIN RAG \n",
    "print(\"--- RESPUESTA SLM (SIN RAG) ---\")\n",
    "res_base = pipe([{\"role\": \"user\", \"content\": pregunta_tfg}], max_new_tokens=150)\n",
    "print(res_base[0]['generated_text'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# 2. Prueba CON RAG \n",
    "print(\"--- RESPUESTA SISTEMA RAG (k=1) ---\")\n",
    "res_rag = ask_rag(pregunta_tfg)\n",
    "print(res_rag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
