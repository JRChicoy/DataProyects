{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b80317",
   "metadata": {},
   "source": [
    "# 03. Despliegue del Modelo e Integración RAG (Fase de Validación)\n",
    "\n",
    "## Objetivos de este Notebook\n",
    "1.  **Carga del SLM:** Desplegar el modelo **Microsoft Phi-3-mini-4k-instruct** en la GPU (RTX 4090).\n",
    "2.  **Conexión Distribuida:** Establecer comunicación con Elasticsearch (alojado en CPU/Valencia) mediante túnel SSH.\n",
    "3.  **Pipeline RAG:** Implementar la función de búsqueda y generación con configuración estricta (**Top-k=1**, **Match Perfecto**) para mitigar alucinaciones.\n",
    "4.  **Validación Técnica:** Realizar pruebas manuales de cordura (Sanity Checks) para asegurar que el sistema recupera noticias reales y admite ignorancia cuando no hay datos.\n",
    "\n",
    "## Requisitos Previos\n",
    "* Túnel SSH activo en terminal: `ssh -L 9250:localhost:9250 javierruiz@valencia...`\n",
    "* Elasticsearch corriendo en el servidor remoto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2b6a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/javierruiz/miniconda3/envs/environment/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware detectado: NVIDIA GeForce RTX 4090\n",
      "Cargando modelo: microsoft/Phi-3-mini-4k-instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|██████████| 195/195 [00:02<00:00, 73.11it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ¡Modelo SLM cargado y listo para inferencia!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# 1. Configuración de Hardware\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Hardware detectado: {torch.cuda.get_device_name(0) if device == 'cuda' else 'CPU'}\")\n",
    "\n",
    "# 2. Selección de Modelo (Empezamos con un SLM para la prueba rápida)\n",
    "MODEL_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "print(f\"Cargando modelo: {MODEL_ID}...\")\n",
    "\n",
    "# 3. Carga del Tokenizer y el Modelo \n",
    "# Importante: Ponemos trust_remote_code=False para usar la versión estable instalada\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",             # Distribuye automáticamente en la GPU\n",
    "    torch_dtype=torch.float16    # Usamos precisión media para ahorrar VRAM y ganar velocidad\n",
    ")\n",
    "\n",
    "# 4. Crear Pipeline de Generación\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"\\n ¡Modelo SLM cargado y listo para inferencia!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73c2c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'do_sample', 'temperature'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=150) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Generando respuesta...\n",
      "\n",
      "==================================================\n",
      " La inflación económica es un fenómeno que se refiere a la tasa a la que el nivel general de precios de bienes y servicios está aumentando, y, por consiguiente, el poder adquisitivo está disminuyendo. En otras palabras, a medida que la inflación aumenta, cada unidad de moneda compra menos bienes y servicios. La inflación puede ser causada por diversos factores, como un aumento en la oferta de dinero, un aumento en los costos de producción, o una disminución en la demanda de bienes y servicios. A largo plazo, una inflación moder\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# 5. Prueba de Inferencia (Sin RAG todavía)\n",
    "# Vamos a ver si el \"cerebro\" funciona por sí solo.\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"¿Podrías explicarme brevemente qué es la inflación económica?\"},\n",
    "]\n",
    "\n",
    "# Configuración de generación\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 150,     # Que no escriba una novela, solo un párrafo\n",
    "    \"return_full_text\": False, # Que no repita la pregunta\n",
    "    \"temperature\": 0.1,        # Creatividad baja (queremos datos, no poesía)\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "print(\" Generando respuesta...\")\n",
    "output = pipe(messages, **generation_args)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(output[0]['generated_text'])\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b788314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PING EXITOSO\n",
      " Conectado a: valencia\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Ahora que las versiones coinciden, la configuración es estándar y sencilla\n",
    "es = Elasticsearch(\"http://127.0.0.1:9250\")\n",
    "\n",
    "try:\n",
    "    if es.ping():\n",
    "        print(\"PING EXITOSO\")\n",
    "        print(f\" Conectado a: {es.info()['name']}\")\n",
    "    else:\n",
    "        print(\"❌ El servidor no responde. ¿Está el túnel abierto?\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f944f6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sistema RAG (k=1) configurado para match perfecto.\n"
     ]
    }
   ],
   "source": [
    "def ask_rag(query, top_k=1): # <--- Forzamos k=1 como pide el profesor\n",
    "    \"\"\"\n",
    "    Función RAG para TFG: Recupera 1 noticia y genera respuesta basada estrictamente en ella.\n",
    "    \"\"\"\n",
    "    # --- A. BÚSQUEDA (Retrieval de 1 sola noticia) ---\n",
    "    search_payload = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"title^5\", \"body\"], # Mucho peso al título\n",
    "                \"fuzziness\": 0 # 0 = Match perfecto (sin errores tipográficos permitidos)\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"title\", \"body\", \"date\", \"source\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = es.search(index=\"noticias_tfg\", body=search_payload)\n",
    "        hits = response['hits']['hits']\n",
    "    except Exception as e:\n",
    "        return f\"Error en Elasticsearch: {e}\"\n",
    "    \n",
    "    if not hits:\n",
    "        return \"⚠️ NO ENCONTRADO: No hay ninguna noticia que coincida exactamente con la búsqueda.\"\n",
    "\n",
    "    # --- B. EXTRACCIÓN DE LA NOTICIA ÚNICA ---\n",
    "    doc = hits[0]['_source']\n",
    "    contexto_unico = f\"\"\"\n",
    "    TITULO: {doc.get('title')}\n",
    "    FECHA: {doc.get('date', 'Desconocida')}\n",
    "    FUENTE: {doc.get('source', 'Desconocida')}\n",
    "    CONTENIDO: {doc.get('body')}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- C. PROMPT PARA MITIGAR ALUCINACIONES ---\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Eres un sistema de verificación de datos (Fact-Checking). \n",
    "        Tu objetivo es responder a la pregunta usando ÚNICAMENTE el texto que te proporciono abajo.\n",
    "\n",
    "        REGLAS CRÍTICAS:\n",
    "        1. Si la respuesta no aparece en el texto, responde exactamente: \"No tengo información suficiente en mis archivos\".\n",
    "        2. No utilices conocimiento externo.\n",
    "        3. No menciones otras noticias que no sean la proporcionada.\n",
    "\n",
    "        ### TEXTO DE REFERENCIA:\n",
    "        {contexto_unico}\n",
    "        \n",
    "        ### PREGUNTA:\n",
    "        {query}\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # --- D. GENERACIÓN ---\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False, # Ponemos False para que sea determinista (siempre responda igual)\n",
    "        temperature=0.0, # Temperatura 0 para evitar inventos\n",
    "    )\n",
    "    \n",
    "    return outputs[0]['generated_text']\n",
    "\n",
    "print(\" Sistema RAG (k=1) configurado para match perfecto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac82dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sistema RAG (k=1) reconfigurado para devolver datos separados.\n"
     ]
    }
   ],
   "source": [
    "def ask_rag(query, top_k=1): \n",
    "    \"\"\"\n",
    "    Función RAG para TFG: Recupera 1 noticia y genera respuesta basada estrictamente en ella.\n",
    "    Retorna un diccionario con los datos separados.\n",
    "    \"\"\"\n",
    "    # --- A. BÚSQUEDA ---\n",
    "    search_payload = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\n",
    "            \"multi_match\": {\n",
    "                \"query\": query,\n",
    "                \"fields\": [\"title^5\", \"body\"],\n",
    "                \"fuzziness\": 0 # 0 = Match perfecto\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"title\", \"body\", \"date\", \"source\"]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = es.search(index=\"noticias_tfg\", body=search_payload)\n",
    "        hits = response['hits']['hits']\n",
    "    except Exception as e:\n",
    "        return {\"error\": f\"Error en Elasticsearch: {e}\"}\n",
    "    \n",
    "    if not hits:\n",
    "        return {\"error\": \" NO ENCONTRADO: No hay ninguna noticia que coincida exactamente con la búsqueda.\"}\n",
    "\n",
    "    # --- B. EXTRACCIÓN ---\n",
    "    doc = hits[0]['_source']\n",
    "    contexto_unico = f\"\"\"\n",
    "    TITULO: {doc.get('title')}\n",
    "    FECHA: {doc.get('date', 'Desconocida')}\n",
    "    FUENTE: {doc.get('source', 'Desconocida')}\n",
    "    CONTENIDO: {doc.get('body')}\n",
    "    \"\"\"\n",
    "\n",
    "    # --- C. PROMPT ---\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        Eres un sistema de verificación de datos (Fact-Checking). \n",
    "        Tu objetivo es responder a la pregunta usando ÚNICAMENTE el texto que te proporciono abajo.\n",
    "\n",
    "        REGLAS CRÍTICAS:\n",
    "        1. Si la respuesta no aparece en el texto, responde exactamente: \"No tengo información suficiente en mis archivos\".\n",
    "        2. No utilices conocimiento externo.\n",
    "        3. No menciones otras noticias que no sean la proporcionada.\n",
    "\n",
    "        ### TEXTO DE REFERENCIA:\n",
    "        {contexto_unico}\n",
    "        \n",
    "        ### PREGUNTA:\n",
    "        {query}\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # --- D. GENERACIÓN ---\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=False,\n",
    "        temperature=0.0, \n",
    "    )\n",
    "    \n",
    "    # --- E. RETORNO ESTRUCTURADO  ---\n",
    "    # Extraemos solo el texto del último mensaje \n",
    "    respuesta_limpia = outputs[0]['generated_text'][-1]['content']\n",
    "    \n",
    "    return {\n",
    "        \"titulo\": doc.get('title'),\n",
    "        \"contenido\": doc.get('body'),\n",
    "        \"fecha\": doc.get('date'),\n",
    "        \"fuente\": doc.get('source'),\n",
    "        \"respuesta_rag\": respuesta_limpia # Aquí va solo el texto que querías\n",
    "    }\n",
    "\n",
    "print(\"✅ Sistema RAG (k=1) reconfigurado para devolver datos separados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e55b54e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=150) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PREGUNTA: ¿Qué aranceles puso Trump a China?\n",
      "\n",
      "--- RESPUESTA SLM (SIN RAG) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " El presidente Donald Trump impuso aranceles a China en un esfuerzo por abordar las disputas comerciales y los desequilibrios comerciales en su contra. El 15 de marzo de 2018, Trump anunció aranceles del 25% sobre las importaciones de productos fabricados en China, citando la necesidad de proteger a los trabajadores estadounidenses y combatir lo que consideraba prácticas comerciales injustas por parte de China. Estos aranceles fueron parte del \"Acuerdo de Aranceles de Estados Unidos y China\" (AUSCA) y se aplicaron inicialmente a 12\n",
      "\n",
      "\n",
      "--- RESPUESTA SISTEMA RAG ---\n",
      "NOTICIA: Aranceles de Trump a China alcanzan el 145%\n",
      "TEXTO: La Casa Blanca precisó que el aumento del 125% de aranceles a China se suma al 20% vigente desde principios de marzo. (...)\n",
      "RESPUESTA:  El presidente Trump puso aranceles a China que suman un 125% al 20% vigente desde principios de marzo.\n"
     ]
    }
   ],
   "source": [
    "pregunta_tfg = \"¿Qué aranceles puso Trump a China?\"\n",
    "\n",
    "print(f\" PREGUNTA: {pregunta_tfg}\\n\")\n",
    "\n",
    "# 1. Prueba SIN RAG \n",
    "print(\"--- RESPUESTA SLM (SIN RAG) ---\")\n",
    "res_base = pipe([{\"role\": \"user\", \"content\": pregunta_tfg}], max_new_tokens=150)\n",
    "print(res_base[0]['generated_text'][-1]['content']) # Solo el texto generado, sin repetir la pregunta\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Ejecutamos el RAG\n",
    "datos = ask_rag(pregunta_tfg)\n",
    "print(\"--- RESPUESTA SISTEMA RAG ---\")\n",
    "# 1. Imprimimos Título\n",
    "print(f\"NOTICIA: {datos['titulo']}\")\n",
    "\n",
    "# 2. Imprimimos Contenido (Recortado un poco para no llenar pantalla)\n",
    "print(f\"TEXTO: {datos['contenido'][:300]} (...)\")\n",
    "\n",
    "# 3. Imprimimos SOLO la respuesta limpia del RAG\n",
    "print(f\"RESPUESTA: {datos['respuesta_rag']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
